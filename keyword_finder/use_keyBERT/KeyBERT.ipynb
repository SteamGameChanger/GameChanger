{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"a0t0AqoQks8C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685464946990,"user_tz":-540,"elapsed":29027,"user":{"displayName":"‍라현아[재학 / 컴퓨터.전자시스템공학전공]","userId":"12633235958779305375"}},"outputId":"557b5a75-ec36-4b3a-8725-9778faf00292"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentence_transformers\n","  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting transformers<5.0.0,>=4.6.0 (from sentence_transformers)\n","  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.65.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.15.2+cu118)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.22.4)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.10.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (3.8.1)\n","Collecting sentencepiece (from sentence_transformers)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub>=0.4.0 (from sentence_transformers)\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.12.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2023.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.27.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (23.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->sentence_transformers) (16.0.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers<5.0.0,>=4.6.0->sentence_transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (8.1.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->sentence_transformers) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers) (8.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers) (2.1.2)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers) (1.3.0)\n","Building wheels for collected packages: sentence_transformers\n","  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125926 sha256=3c1adb90426777a65d7ddb10172160055effcb694bf145b4169ba11a7f2a8824\n","  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n","Successfully built sentence_transformers\n","Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers, sentence_transformers\n","Successfully installed huggingface-hub-0.14.1 sentence_transformers-2.2.2 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.29.2\n"]}],"source":["!pip install sentence_transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"2X8N_KE9xU3X","executionInfo":{"status":"ok","timestamp":1685464958362,"user_tz":-540,"elapsed":11385,"user":{"displayName":"‍라현아[재학 / 컴퓨터.전자시스템공학전공]","userId":"12633235958779305375"}}},"outputs":[],"source":["import itertools\n","import numpy as np\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sentence_transformers import SentenceTransformer"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"YlEBSD_5ySyR","executionInfo":{"status":"ok","timestamp":1685464958364,"user_tz":-540,"elapsed":13,"user":{"displayName":"‍라현아[재학 / 컴퓨터.전자시스템공학전공]","userId":"12633235958779305375"}}},"outputs":[],"source":["doc = \"\"\"\n","         Supervised learning is the machine learning task of \n","         learning a function that maps an input to an output based \n","         on example input-output pairs.[1] It infers a function \n","         from labeled training data consisting of a set of \n","         training examples.[2] In supervised learning, each \n","         example is a pair consisting of an input object \n","         (typically a vector) and a desired output value (also \n","         called the supervisory signal). A supervised learning \n","         algorithm analyzes the training data and produces an \n","         inferred function, which can be used for mapping new \n","         examples. An optimal scenario will allow for the algorithm \n","         to correctly determine the class labels for unseen \n","         instances. This requires the learning algorithm to  \n","         generalize from the training data to unseen situations \n","         in a 'reasonable' way (see inductive bias).\n","      \"\"\""]},{"cell_type":"code","execution_count":4,"metadata":{"id":"O3aaau6MyXLP","executionInfo":{"status":"ok","timestamp":1685464958364,"user_tz":-540,"elapsed":11,"user":{"displayName":"‍라현아[재학 / 컴퓨터.전자시스템공학전공]","userId":"12633235958779305375"}}},"outputs":[],"source":["n_gram_range = (3,3)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"VJtuiOiNluUR","executionInfo":{"status":"ok","timestamp":1685465262442,"user_tz":-540,"elapsed":20,"user":{"displayName":"‍라현아[재학 / 컴퓨터.전자시스템공학전공]","userId":"12633235958779305375"}}},"outputs":[],"source":["class DoKeyBERT:\n","    def __init__(self, doc, n_gram_range):\n","        self.doc = doc\n","        self.n_gram_range = n_gram_range\n","        self.stop_words = 'english'\n","\n","        self.count = None\n","        self.candidates = []\n","\n","        self.model = None\n","        self.doc_embedding = None\n","        self.candidate_embeddings = None\n","\n","        \"\"\"\n","        top_n = [5, 10]\n","        nr_candidates = [10, 20]\n","        diversity = [0.2, 0.5, 0.7]\n","        \"\"\"\n","\n","    def vectorize(self):\n","        self.count = CountVectorizer(ngram_range=self.n_gram_range, stop_words=self.stop_words).fit([self.doc])\n","        self.candidates = self.count.get_feature_names_out()\n","\n","    def embed(self):\n","        self.model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n","        self.doc_embedding = self.model.encode([self.doc])\n","        self.candidate_embeddings = self.model.encode(self.candidates)\n","\n","    def max_sum_sim(self, top_n, nr_candidates):\n","        # 문서와 각 키워드들 간의 유사도\n","        distances = cosine_similarity(self.doc_embedding, self.candidate_embeddings)\n","\n","        # 각 키워드들 간의 유사도\n","        distances_candidates = cosine_similarity(self.candidate_embeddings, \n","                                                self.candidate_embeddings)\n","\n","        # 코사인 유사도에 기반하여 키워드들 중 상위 top_n개의 단어를 pick.\n","        words_idx = list(distances.argsort()[0][-nr_candidates:])\n","        words_vals = [self.candidates[index] for index in words_idx]\n","        distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n","\n","        # 각 키워드들 중에서 가장 덜 유사한 키워드들간의 조합을 계산\n","        min_sim = np.inf\n","        candidate = None\n","        for combination in itertools.combinations(range(len(words_idx)), top_n):\n","            sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n","            if sim < min_sim:\n","                candidate = combination\n","                min_sim = sim\n","\n","        return [words_vals[idx] for idx in candidate]\n","\n","    def mmr(self, top_n, diversity):\n","        words = self.candidates\n","        \n","        # 문서와 각 키워드들 간의 유사도가 적혀있는 리스트\n","        word_doc_similarity = cosine_similarity(self.candidate_embeddings, self.doc_embedding)\n","\n","        # 각 키워드들 간의 유사도\n","        word_similarity = cosine_similarity(self.candidate_embeddings)\n","\n","        # 문서와 가장 높은 유사도를 가진 키워드의 인덱스를 추출.\n","        # 만약, 2번 문서가 가장 유사도가 높았다면\n","        # keywords_idx = [2]\n","        keywords_idx = [np.argmax(word_doc_similarity)]\n","\n","        # 가장 높은 유사도를 가진 키워드의 인덱스를 제외한 문서의 인덱스들\n","        # 만약, 2번 문서가 가장 유사도가 높았다면\n","        # ==> candidates_idx = [0, 1, 3, 4, 5, 6, 7, 8, 9, 10 ... 중략 ...]\n","        candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n","\n","        # 최고의 키워드는 이미 추출했으므로 top_n-1번만큼 아래를 반복.\n","        # ex) top_n = 5라면, 아래의 loop는 4번 반복됨.\n","        for _ in range(top_n - 1):\n","            candidate_similarities = word_doc_similarity[candidates_idx, :]\n","            target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n","\n","            # MMR을 계산\n","            mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n","            mmr_idx = candidates_idx[np.argmax(mmr)]\n","\n","            # keywords & candidates를 업데이트\n","            keywords_idx.append(mmr_idx)\n","            candidates_idx.remove(mmr_idx)\n","\n","        return [words[idx] for idx in keywords_idx]\n","\n","    def run(self, top_n, nr_candidates, diversity):\n","\n","        self.vectorize()\n","        self.embed()\n","\n","        print(\"Max Sum Similarity Keywords for top_n={}, nr_candidates={}\".format(top_n, nr_candidates))\n","        print(self.max_sum_sim(top_n, nr_candidates))\n","        print(\"MMR Keywords for top_n={}, diversity={}\".format(top_n, diversity))\n","        print(self.mmr(top_n, diversity))\n","\n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"uBld_fmoyqls","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685465274274,"user_tz":-540,"elapsed":11850,"user":{"displayName":"‍라현아[재학 / 컴퓨터.전자시스템공학전공]","userId":"12633235958779305375"}},"outputId":"4ce135f0-e446-4be7-c7b1-e736bc395219"},"outputs":[{"output_type":"stream","name":"stdout","text":["Max Sum Similarity Keywords for top_n=10, nr_candidates=20\n","['set training examples', 'training data produces', 'generalize training data', 'supervised learning example', 'analyzes training data', 'machine learning task', 'requires learning algorithm', 'learning function maps', 'supervised learning algorithm', 'learning machine learning']\n","MMR Keywords for top_n=10, diversity=0.5\n","['algorithm generalize training', 'supervised learning algorithm', 'learning machine learning', 'learning algorithm analyzes', 'mapping new examples', 'algorithm correctly determine', 'learning algorithm generalize', 'learning function maps', 'algorithm analyzes training', 'requires learning algorithm']\n"]}],"source":["keyBERT = DoKeyBERT(doc, n_gram_range)\n","top_n = 10\n","nr_candidates = 20\n","diversity = 0.5\n","keyBERT.run(top_n, nr_candidates, diversity)"]},{"cell_type":"code","source":[],"metadata":{"id":"wq1lqhz0kAfp"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}